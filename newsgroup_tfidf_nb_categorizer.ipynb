{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:34:14.611772Z",
     "start_time": "2017-12-30T04:34:14.607615Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:34:14.949727Z",
     "start_time": "2017-12-30T04:34:14.944939Z"
    }
   },
   "outputs": [],
   "source": [
    "wp_tokenizer = WordPunctTokenizer()\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL', 'N LEMMATIZER', 'V LEMMATIZER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:34:15.394416Z",
     "start_time": "2017-12-30T04:34:15.391580Z"
    }
   },
   "outputs": [],
   "source": [
    "formatted_text = '{:>15}' * (len(stemmer_names)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:34:18.258540Z",
     "start_time": "2017-12-30T04:34:18.253202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      INPUT WORD         PORTER      LANCASTER       SNOWBALL   N LEMMATIZER   V LEMMATIZER \n",
      " =====================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), \n",
    "     '\\n', '='*85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:34:18.974464Z",
     "start_time": "2017-12-30T04:34:18.965223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          hello          hello          hello          hello          hello          hello\n",
      "          found          found          found          found          found           find\n",
      "         trying            tri            try            tri         trying            try\n",
      "            ran            ran            ran            ran            ran            run\n",
      "         called           call            cal           call         called           call\n",
      "           code           code            cod           code           code           code\n"
     ]
    }
   ],
   "source": [
    "input_words=['hello', 'found', 'trying', 'ran', 'called', 'code']\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), lancaster.stem(word), snowball.stem(word),\n",
    "             lemmatizer.lemmatize(word, pos='n'), lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:34:20.352225Z",
     "start_time": "2017-12-30T04:34:20.349297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:44.093740Z",
     "start_time": "2017-12-30T04:39:44.083277Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output=[]\n",
    "    cur_chunk = []\n",
    "    count=0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "    if len(cur_chunk) > 0:\n",
    "        output.append(' '.join(cur_chunk))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:44.532634Z",
     "start_time": "2017-12-30T04:39:44.489317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data  = ' '.join(brown.words()[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:44.861501Z",
     "start_time": "2017-12-30T04:39:44.856780Z"
    }
   },
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "text_chunks = chunker(input_data, chunk_size)\n",
    "#print(text_chunks[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T01:42:52.379316Z",
     "start_time": "2017-12-30T01:42:52.376670Z"
    }
   },
   "source": [
    "## Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:46.202098Z",
     "start_time": "2017-12-30T04:39:46.193092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "total words 5\n",
      "{'index': 4, 'text': \"the Education courses . Fifty-three of the 150 representatives immediately joined Grover as co-signers of the proposal . Paris , Texas ( sp. ) -- The board of regents of Paris Junior College has named Dr. Clarence Charles Clark of Hays , Kan. as the school's new president . Dr. Clark will succeed Dr. J. R. McLemore , who will retire at the close of the present school term . Dr. Clark holds an earned Doctor of Education degree from the University of Oklahoma . He also received a Master of Science degree from Texas A & I College and a Bachelor of Science degree from Southwestern State College , Weatherford , Okla. . In addition , Dr. Clark has studied at Rhode Island State College and Massachusetts Institute of Technology . During his college career , Dr. Clark was captain of his basketball team and was a football letterman . Dr. Clark has served as teacher and principal in Oklahoma high schools , as teacher and athletic director at Raymondville , Texas , High School , as an instructor at the University of Oklahoma , and as an associate professor of education at Fort Hays , Kan. , State College . He has served as a border patrolman and was in the Signal Corps of the U.S. Army . Denton , Texas ( sp. ) -- Principals of the 13 schools in the Denton Independent School District have been re-elected for the 1961-62 session upon the recommendation of Supt. Chester O. Strickland . State and federal legislation against racial discrimination in employment was called for yesterday in a report of a `` blue ribbon '' citizens committee on the aid to dependent children program . The report , culminating a year long study of the ADC program in Cook county by a New York City welfare consulting firm , listed 10 long range recommendations designed to reduce the soaring ADC case load . The report called racial discrimination in employment `` one of the most serious causes of family breakdown , desertion , and ADC dependency '' . `` Must solve problem '' The monthly cost of ADC to more than 100,000 recipients in the county is 4.4 million dollars , said C. Virgil Martin , president of Carson Pirie Scott & Co. , committee chairman . `` We must solve the problems which have forced these people to depend upon ADC for subsistence '' , Martin said . The volume of ADC cases will decrease , Martin reported , when the community is able to deal effectively with two problems : Relatively limited skills and discrimination in employment because of color . These , he said , are `` two of the principal underlying causes for family breakups leading to ADC '' . Calls for extension Other recommendations made by the committee are : Extension of the ADC program to all children in need living with any relatives , including both parents , as a means of preserving family unity . Research projects as soon as possible on the causes and prevention of dependency and illegitimacy . Several defendants in the Summerdale police burglary trial made statements indicating their guilt at the time of their arrest , Judge James B. Parsons was told in Criminal court yesterday . The disclosure by Charles Bellows , chief defense counsel , startled observers and was viewed as the prelude to a quarrel between the six attorneys representing the eight former policemen now on trial . Bellows made the disclosure when he asked Judge Parsons to grant his client , Alan Clements , 30 , a separate trial . Bellows made the request while the all-woman jury was out of the courtroom . Fears prejudicial aspects `` The statements may be highly prejudicial to my client '' , Bellows told the court . `` Some of the defendants strongly indicated they knew they were receiving stolen property . It is impossible to get a fair trial when some of the defendants made statements involving themselves and others '' . Judge Parsons leaned over the bench and inquired , `` You mean some of the defendants made statements admitting this '' ? ? `` Yes , your honor '' , replied Bellows . `` What this amounts to , if true , is that there will be a free-for-all fight in this case . There is a conflict among the defendants '' . Washington , July 24 -- President Kennedy today pushed aside other White House business to devote all his time and attention to working on the Berlin crisis address he will deliver tomorrow night to the American people over nationwide television and radio . The President spent much of the week-end at his summer home on Cape Cod writing the first drafts of portions of the address with the help of White House aids in Washington with whom he talked by telephone . Shortly after the Chief Executive returned to Washington in midmorning from Hyannis Port , Mass. , a White House spokesman said the address text still had `` quite a way to go '' toward completion . Decisions are made Asked to elaborate , Pierre Salinger , White House press secretary , replied , `` I would say it's got to go thru several more drafts '' . Salinger said the work President Kennedy , advisers , and members of his staff were doing on the address involved composition and wording , rather than last minute decisions on administration plans to meet the latest Berlin crisis precipitated by Russia's demands and proposals for the city . The last 10 cases in the investigation of the Nov. 8 election were dismissed yesterday by Acting Judge John M. Karns , who charged that the prosecution obtained evidence `` by unfair and fundamentally illegal means '' . Karns said that the cases involved a matter `` of even greater significance than the guilt or innocence '' of the 50 persons . He\"}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "chunks=[]\n",
    "for count, chunk in enumerate(text_chunks):\n",
    "    print(count)\n",
    "    d = {'index': count, 'text': chunk}\n",
    "    chunks.append(d)\n",
    "print('total words', len(chunks))\n",
    "print(chunks[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:47.491467Z",
     "start_time": "2017-12-30T04:39:47.478899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary: (39,)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=5, max_df=25)\n",
    "document_term_matrix = count_vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "vocabulary = np.array(count_vectorizer.get_feature_names())\n",
    "print('\\nVocabulary:', vocabulary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:48.667720Z",
     "start_time": "2017-12-30T04:39:48.662738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document shape (5, 39)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print('document shape', document_term_matrix.shape)\n",
    "print(type(document_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:49.348408Z",
     "start_time": "2017-12-30T04:39:49.321939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         Word          C1          C2          C3          C4          C5\n",
      "        also           2           1           2           1           1\n",
      "          an           4           1           3           4           3\n",
      "         and          27           7          16          11          21\n",
      "         are           3           2           1           2           3\n",
      "          as           7           6           2           2          11\n",
      "          at           4           1           6           1           7\n",
      "          be           7          13           7           7           2\n",
      "          by           4           4           4          18           7\n",
      "      county           6           7           4           2           2\n",
      "         for          10          12           8          11           7\n",
      "        from           3           1           2           3           4\n",
      "         had           2           1           1           1           1\n",
      "         has           6           4           1           1           4\n",
      "        have           2           2           1           3           2\n",
      "          he           2           2          12           1           7\n",
      "          in          18          20          11          16          17\n",
      "          is           3           9           3           6           5\n",
      "          it           9           9          12           4           2\n",
      "        more           1           2           4           1           2\n",
      "          of          36          25          31          38          42\n",
      "          on           5           5          10           8           7\n",
      "         one           2           2           1           4           1\n",
      "          or           2           2           2           2           1\n",
      "        said          14           6          11           4           6\n",
      "       state           3           9           4           5           4\n",
      "        than           1           1           1           1           3\n",
      "        that          16          11           5           7           3\n",
      "         the          82          61          57          59          67\n",
      "        they           2           1           1           1           2\n",
      "        this           6           2           2           3           3\n",
      "          to          14          36          29          25          20\n",
      "         two           2           1           1           2           2\n",
      "         was           6           7          11           5           7\n",
      "        when           1           2           1           2           3\n",
      "       which           7           6           6           4           1\n",
      "        will           4          10           1           1           5\n",
      "        with           3           1           4           2           4\n",
      "       would           1           7          12          15           1\n",
      "        year           1           1           2           2           1\n"
     ]
    }
   ],
   "source": [
    "chunk_names =[]\n",
    "for i in range(len(text_chunks)):\n",
    "    chunk_names.append('C' + str(i+1))\n",
    "formatted_text = '{:>12}' * (len(chunk_names) + 1)\n",
    "\n",
    "print('\\n', formatted_text.format('Word', *chunk_names))\n",
    "for word, item in zip(vocabulary,document_term_matrix.T):\n",
    "    # item is a csr_matrix\n",
    "    #print('item shape', item.shape)\n",
    "    #print('item data shape', item.data.shape)\n",
    "    output = [word] + [str(freq) for freq in item.data]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T02:30:28.148592Z",
     "start_time": "2017-12-30T02:30:28.146143Z"
    }
   },
   "source": [
    "## Category Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:50.978690Z",
     "start_time": "2017-12-30T04:39:50.974703Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:51.482357Z",
     "start_time": "2017-12-30T04:39:51.478881Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_map={'talk.politics.misc':'Politics',\n",
    "             'rec.autos': 'Autos',\n",
    "             'rec.sport.hockey': 'Hockey',\n",
    "              'sci.electronics': 'Electronics',\n",
    "              'sci.med': 'Mecicine'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:52.305991Z",
     "start_time": "2017-12-30T04:39:52.032130Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = fetch_20newsgroups(subset='train',\n",
    "                                  categories=category_map.keys(),\n",
    "                                  shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:53.440865Z",
     "start_time": "2017-12-30T04:39:52.745635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing data shape (2844, 40321)\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer=CountVectorizer()\n",
    "train_tc=count_vectorizer.fit_transform(training_data.data)\n",
    "print('traing data shape', train_tc.shape)\n",
    "print(set(training_data.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:53.465438Z",
     "start_time": "2017-12-30T04:39:53.442781Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "train_tfidf = tfidf.fit_transform(train_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:53.743643Z",
     "start_time": "2017-12-30T04:39:53.730821Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = MultinomialNB().fit(train_tfidf, training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:39:54.221017Z",
     "start_time": "2017-12-30T04:39:54.218078Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = [\n",
    "    'Mobile computing is everywhere',\n",
    "    'Players work hard to win the game',\n",
    "    'A president needs to pay attention to people need',\n",
    "    'When driving on a high way, be careful!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:31:41.690137Z",
     "start_time": "2017-12-30T04:31:41.685023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_tc = count_vectorizer.transform(input_data)\n",
    "input_tfidf = tfidf.transform(input_tc)\n",
    "predictions = classifier.predict(input_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-30T04:31:43.701443Z",
     "start_time": "2017-12-30T04:31:43.693372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Mobile computing is everywhere \n",
      "Preditect category: Electronics\n",
      "Input: Players work hard to win the game \n",
      "Preditect category: Hockey\n",
      "Input: A president needs to pay attention to people need \n",
      "Preditect category: Politics\n",
      "Input: When driving on a high way, be careful! \n",
      "Preditect category: Autos\n"
     ]
    }
   ],
   "source": [
    "for sent, category in zip(input_data, predictions):\n",
    "    print('Input:', sent, '\\nPreditect category:',\n",
    "         category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:g]",
   "language": "python",
   "name": "conda-env-g-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
